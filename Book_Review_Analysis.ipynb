{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":63679,"databundleVersionId":6956711,"sourceType":"competition"}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T15:25:20.703790Z","iopub.execute_input":"2023-12-15T15:25:20.704254Z","iopub.status.idle":"2023-12-15T15:25:21.184263Z","shell.execute_reply.started":"2023-12-15T15:25:20.704216Z","shell.execute_reply":"2023-12-15T15:25:21.183023Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/to-read-or-not-to-read/sample_submission.csv\n/kaggle/input/to-read-or-not-to-read/train.csv\n/kaggle/input/to-read-or-not-to-read/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"#Loading Dataset\ndata = pd.read_csv('/kaggle/input/to-read-or-not-to-read/train.csv') ","metadata":{"execution":{"iopub.status.busy":"2023-12-15T15:25:21.186616Z","iopub.execute_input":"2023-12-15T15:25:21.187111Z","iopub.status.idle":"2023-12-15T15:25:40.604434Z","shell.execute_reply.started":"2023-12-15T15:25:21.187078Z","shell.execute_reply":"2023-12-15T15:25:40.603249Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Importing required library\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report, f1_score\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport re\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T15:25:40.606103Z","iopub.execute_input":"2023-12-15T15:25:40.606472Z","iopub.status.idle":"2023-12-15T15:25:42.007597Z","shell.execute_reply.started":"2023-12-15T15:25:40.606441Z","shell.execute_reply":"2023-12-15T15:25:42.005653Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#to download the necessary data files for the NLTK library, specifically the tokenization models.\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2023-12-15T15:25:42.009493Z","iopub.execute_input":"2023-12-15T15:25:42.010083Z","iopub.status.idle":"2023-12-15T15:25:42.194083Z","shell.execute_reply.started":"2023-12-15T15:25:42.010036Z","shell.execute_reply":"2023-12-15T15:25:42.193141Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"#Stopwords\n#used for stemming words in natural language processing.\nstop_words = set(stopwords.words('english'))\nps = PorterStemmer()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T15:25:42.196930Z","iopub.execute_input":"2023-12-15T15:25:42.197266Z","iopub.status.idle":"2023-12-15T15:25:42.204975Z","shell.execute_reply.started":"2023-12-15T15:25:42.197238Z","shell.execute_reply":"2023-12-15T15:25:42.203731Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#Removing URL, HTML Tags, Non-Alphabetic & Stopwords.\ndef clean_text(text):\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n    text = re.sub(r'[^A-Za-z]', ' ', text)  # Remove non-alphabetic characters\n    text = ' '.join([ps.stem(word) for word in word_tokenize(text) if word.lower() not in stop_words])  # Remove stopwords and apply stemming\n    return text","metadata":{"execution":{"iopub.status.busy":"2023-12-15T15:25:42.206553Z","iopub.execute_input":"2023-12-15T15:25:42.206987Z","iopub.status.idle":"2023-12-15T15:25:42.215231Z","shell.execute_reply.started":"2023-12-15T15:25:42.206880Z","shell.execute_reply":"2023-12-15T15:25:42.213737Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#Calling The clean_text function on dataset.\ndata['cleaned_review'] = data['review_text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T15:25:42.216823Z","iopub.execute_input":"2023-12-15T15:25:42.217140Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Taking the required data.\ndata=data[['user_id','cleaned_review','rating','n_votes','n_comments']]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Splitting the data for train and test\ntrain_data, test_data, train_labels, test_labels = train_test_split(\n    data[['user_id','cleaned_review','n_votes','n_comments']], data['rating'], test_size=0.1, random_state=20\n)#0.2 #42\n##new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# text_features = 'cleaned_review'\n# user_id_feature = 'user_id'\ntext_features = 'cleaned_review'\nuser_id_feature = 'user_id'\nvotes_feature = 'n_votes'\ncomments_feature = 'n_comments'\n##new","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" #ColumnTransformer is a powerful tool for applying different transformations to different columns in a dataset\n#-----------------------------------------------------------------------------------------------------------------------------------\n\n#TfidfVectorizer. This is a text vectorizer that converts a collection of raw documents to a matrix of TF-IDF features\n#------------------------------------------------------------------------------------------------------------------------------------\n\n# The most popular approach is using the Term Frequency-Inverse Document Frequency (TF-IDF) technique.\n\n# Term Frequency (TF) = (Number of times term t appears in a document)/(Number of terms in the document)\n\n# Inverse Document Frequency (IDF) = log(N/n), where, N is the number of documents and n is the number of documents a \n#term t has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. \n#Thus having the effect of highlighting words that are distinct.\n\n# We calculate TF-IDF value of a term as = TF * IDF\n#------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n# One-hot encoding is a process used to convert categorical data, represented as integer labels, \n# into a binary matrix where each category is represented by a binary column.","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Applying the pipeline\nfrom sklearn.preprocessing import OneHotEncoder\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', TfidfVectorizer(max_features=25000, ngram_range=(1, 2)), text_features),\n        ('user_id', OneHotEncoder(handle_unknown='ignore'), [user_id_feature]) , # One-hot encode 'user_id'\n        ('votes', 'passthrough', [votes_feature]),\n        ('comments', 'passthrough', [comments_feature])   \n    ]\n)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a pipeline is a way to streamline a lot of routine processes, which can be particularly useful for machine learning workflows. \n# A pipeline bundles together a sequence of data processing steps and a model into a single object. \n# This ensures that the entire workflow, including data preprocessing and model training, can be treated as a single unit.\n\npipeline = Pipeline([\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression(max_iter=7000, random_state=42, multi_class='multinomial'))  # Logistic Regression for multiclass classification\n])#1000 #42","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training The Model using train data\npipeline.fit(train_data, train_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Testing the model using 20% Test data\npredictions = pipeline.predict(test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Looking for the accuracy score and classification report\naccuracy = accuracy_score(test_labels, predictions)\nclassification_rep = classification_report(test_labels, predictions)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Printing the accuracy and classification report\nprint(f'Accuracy: {accuracy:.2f}')\nprint('Classification Report:')\nprint(classification_rep)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the given test data\ngiven_test_data = pd.read_csv('/kaggle/input/to-read-or-not-to-read/test.csv', usecols=['user_id','review_text','n_votes','n_comments','review_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Cleaning the data\ngiven_test_data['cleaned_review'] = given_test_data['review_text'].apply(clean_text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_text_features = 'cleaned_review'\ntest_user_id_feature = 'user_id'\ntest_votes_feature = 'n_votes'\ntest_comments_feature = 'n_comments'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"given_test_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"given_test_data = given_test_data[['user_id','cleaned_review','n_votes','n_comments']]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting the label for given test data.\ntest_predictions=pipeline.predict(given_test_data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Loading the sample submission\nsubmission = pd.read_csv('/kaggle/input/to-read-or-not-to-read/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replacing the rating column with our prediction \nsubmission['rating']=test_predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Saving the csv.\nsubmission.to_csv('submission.csv',index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}